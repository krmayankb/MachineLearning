{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0kJgWSH6SFFx",
        "outputId": "e4f89d59-d17c-4f66-d3c1-19a620a69196"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os \n",
        "import matplotlib.pyplot as plt \n",
        "import cv2 as cv \n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqLxSzwuSFGF"
      },
      "source": [
        "## Dataset Details - Standford's GloVe pre-trained word vectors\n",
        "\n",
        "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus. The GloVe pre-trained word vectors dataset contains English word vectors pre-trained on the combined Wikipedia 2014 + Gigaword 5th Edition corpora (6B tokens, 400K vocab). All tokens are in lowercase. This dataset contains 50-dimensional, 100-dimensional and 200-dimensional pre trained word vectors. In this problem we are going to use the 50-dimensional dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0qDUTSGSFGG"
      },
      "source": [
        "## \\# 0. Get an overview on what Glove is\n",
        "Read up the documentation on glove embeddings, esp. where it gets applied here: https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyMOduHlSFGJ"
      },
      "source": [
        "## Load Dataset\n",
        "Let's load the dataset first. Each row is indexed as a word vector. Dimension of word vectors is 50. How many words are there in this dataset? Print a few words and see what they are. You don't need to code anything here, just understand the data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SZ6WYR5SFGJ",
        "outputId": "40ca62c3-3683-49fa-837d-4c28935c25d8",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape - Rows: 400000, Cols: 50\n",
            "print a few words in the dataset: ['be', 'has', 'are', 'have', 'but', 'were', 'not', 'this', 'who', 'they']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "# Load GloVe pre-trained vectors \n",
        "# local_file1=\"./glove/glove.6B.50d.txt\" # Make sure this file exists!\n",
        "filePATH = '/content/drive/MyDrive/Machine_Learning/HW4/glove.6B.50d.txt' \n",
        "df = pd.read_csv(filePATH,sep=' ', index_col=0, header=None, engine='python', on_bad_lines = 'skip', quoting = csv.QUOTE_NONE)\n",
        "print(\"Dataset shape - Rows: %d, Cols: %d\" % (df.shape[0], df.shape[1]))\n",
        "\n",
        "words = list(df.index)\n",
        "print(\"print a few words in the dataset:\", words[30:40])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTRsD5RgSFGJ"
      },
      "source": [
        "## \\# 1. Print the first few 11 rows of the pandas data frame below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "BRAhSd2zSFGJ",
        "outputId": "6f3b4a26-e37b-4cd7-d711-c6ca6acca198"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f23ee894-8073-4926-a39c-0f9b7d09c1c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0.418000</td>\n",
              "      <td>0.249680</td>\n",
              "      <td>-0.41242</td>\n",
              "      <td>0.121700</td>\n",
              "      <td>0.345270</td>\n",
              "      <td>-0.044457</td>\n",
              "      <td>-0.49688</td>\n",
              "      <td>-0.178620</td>\n",
              "      <td>-0.000660</td>\n",
              "      <td>-0.656600</td>\n",
              "      <td>0.278430</td>\n",
              "      <td>-0.147670</td>\n",
              "      <td>-0.55677</td>\n",
              "      <td>0.146580</td>\n",
              "      <td>-0.00951</td>\n",
              "      <td>0.011658</td>\n",
              "      <td>0.102040</td>\n",
              "      <td>-0.127920</td>\n",
              "      <td>-0.84430</td>\n",
              "      <td>-0.121810</td>\n",
              "      <td>-0.016801</td>\n",
              "      <td>-0.332790</td>\n",
              "      <td>-0.155200</td>\n",
              "      <td>-0.231310</td>\n",
              "      <td>-0.191810</td>\n",
              "      <td>-1.8823</td>\n",
              "      <td>-0.767460</td>\n",
              "      <td>0.099051</td>\n",
              "      <td>-0.421250</td>\n",
              "      <td>-0.195260</td>\n",
              "      <td>4.0071</td>\n",
              "      <td>-0.185940</td>\n",
              "      <td>-0.522870</td>\n",
              "      <td>-0.316810</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.007445</td>\n",
              "      <td>0.177780</td>\n",
              "      <td>-0.158970</td>\n",
              "      <td>0.012041</td>\n",
              "      <td>-0.054223</td>\n",
              "      <td>-0.298710</td>\n",
              "      <td>-0.157490</td>\n",
              "      <td>-0.347580</td>\n",
              "      <td>-0.045637</td>\n",
              "      <td>-0.442510</td>\n",
              "      <td>0.187850</td>\n",
              "      <td>0.002785</td>\n",
              "      <td>-0.184110</td>\n",
              "      <td>-0.115140</td>\n",
              "      <td>-0.785810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>,</th>\n",
              "      <td>0.013441</td>\n",
              "      <td>0.236820</td>\n",
              "      <td>-0.16899</td>\n",
              "      <td>0.409510</td>\n",
              "      <td>0.638120</td>\n",
              "      <td>0.477090</td>\n",
              "      <td>-0.42852</td>\n",
              "      <td>-0.556410</td>\n",
              "      <td>-0.364000</td>\n",
              "      <td>-0.239380</td>\n",
              "      <td>0.130010</td>\n",
              "      <td>-0.063734</td>\n",
              "      <td>-0.39575</td>\n",
              "      <td>-0.481620</td>\n",
              "      <td>0.23291</td>\n",
              "      <td>0.090201</td>\n",
              "      <td>-0.133240</td>\n",
              "      <td>0.078639</td>\n",
              "      <td>-0.41634</td>\n",
              "      <td>-0.154280</td>\n",
              "      <td>0.100680</td>\n",
              "      <td>0.488910</td>\n",
              "      <td>0.312260</td>\n",
              "      <td>-0.125200</td>\n",
              "      <td>-0.037512</td>\n",
              "      <td>-1.5179</td>\n",
              "      <td>0.126120</td>\n",
              "      <td>-0.024420</td>\n",
              "      <td>-0.042961</td>\n",
              "      <td>-0.283510</td>\n",
              "      <td>3.5416</td>\n",
              "      <td>-0.119560</td>\n",
              "      <td>-0.014533</td>\n",
              "      <td>-0.149900</td>\n",
              "      <td>0.218640</td>\n",
              "      <td>-0.334120</td>\n",
              "      <td>-0.138720</td>\n",
              "      <td>0.318060</td>\n",
              "      <td>0.703580</td>\n",
              "      <td>0.448580</td>\n",
              "      <td>-0.080262</td>\n",
              "      <td>0.630030</td>\n",
              "      <td>0.321110</td>\n",
              "      <td>-0.467650</td>\n",
              "      <td>0.227860</td>\n",
              "      <td>0.360340</td>\n",
              "      <td>-0.378180</td>\n",
              "      <td>-0.566570</td>\n",
              "      <td>0.044691</td>\n",
              "      <td>0.303920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>0.151640</td>\n",
              "      <td>0.301770</td>\n",
              "      <td>-0.16763</td>\n",
              "      <td>0.176840</td>\n",
              "      <td>0.317190</td>\n",
              "      <td>0.339730</td>\n",
              "      <td>-0.43478</td>\n",
              "      <td>-0.310860</td>\n",
              "      <td>-0.449990</td>\n",
              "      <td>-0.294860</td>\n",
              "      <td>0.166080</td>\n",
              "      <td>0.119630</td>\n",
              "      <td>-0.41328</td>\n",
              "      <td>-0.423530</td>\n",
              "      <td>0.59868</td>\n",
              "      <td>0.288250</td>\n",
              "      <td>-0.115470</td>\n",
              "      <td>-0.041848</td>\n",
              "      <td>-0.67989</td>\n",
              "      <td>-0.250630</td>\n",
              "      <td>0.184720</td>\n",
              "      <td>0.086876</td>\n",
              "      <td>0.465820</td>\n",
              "      <td>0.015035</td>\n",
              "      <td>0.043474</td>\n",
              "      <td>-1.4671</td>\n",
              "      <td>-0.303840</td>\n",
              "      <td>-0.023441</td>\n",
              "      <td>0.305890</td>\n",
              "      <td>-0.217850</td>\n",
              "      <td>3.7460</td>\n",
              "      <td>0.004228</td>\n",
              "      <td>-0.184360</td>\n",
              "      <td>-0.462090</td>\n",
              "      <td>0.098329</td>\n",
              "      <td>-0.119070</td>\n",
              "      <td>0.239190</td>\n",
              "      <td>0.116100</td>\n",
              "      <td>0.417050</td>\n",
              "      <td>0.056763</td>\n",
              "      <td>-0.000064</td>\n",
              "      <td>0.068987</td>\n",
              "      <td>0.087939</td>\n",
              "      <td>-0.102850</td>\n",
              "      <td>-0.139310</td>\n",
              "      <td>0.223140</td>\n",
              "      <td>-0.080803</td>\n",
              "      <td>-0.356520</td>\n",
              "      <td>0.016413</td>\n",
              "      <td>0.102160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.708530</td>\n",
              "      <td>0.570880</td>\n",
              "      <td>-0.47160</td>\n",
              "      <td>0.180480</td>\n",
              "      <td>0.544490</td>\n",
              "      <td>0.726030</td>\n",
              "      <td>0.18157</td>\n",
              "      <td>-0.523930</td>\n",
              "      <td>0.103810</td>\n",
              "      <td>-0.175660</td>\n",
              "      <td>0.078852</td>\n",
              "      <td>-0.362160</td>\n",
              "      <td>-0.11829</td>\n",
              "      <td>-0.833360</td>\n",
              "      <td>0.11917</td>\n",
              "      <td>-0.166050</td>\n",
              "      <td>0.061555</td>\n",
              "      <td>-0.012719</td>\n",
              "      <td>-0.56623</td>\n",
              "      <td>0.013616</td>\n",
              "      <td>0.228510</td>\n",
              "      <td>-0.143960</td>\n",
              "      <td>-0.067549</td>\n",
              "      <td>-0.381570</td>\n",
              "      <td>-0.236980</td>\n",
              "      <td>-1.7037</td>\n",
              "      <td>-0.866920</td>\n",
              "      <td>-0.267040</td>\n",
              "      <td>-0.258900</td>\n",
              "      <td>0.176700</td>\n",
              "      <td>3.8676</td>\n",
              "      <td>-0.161300</td>\n",
              "      <td>-0.132730</td>\n",
              "      <td>-0.688810</td>\n",
              "      <td>0.184440</td>\n",
              "      <td>0.005246</td>\n",
              "      <td>-0.338740</td>\n",
              "      <td>-0.078956</td>\n",
              "      <td>0.241850</td>\n",
              "      <td>0.365760</td>\n",
              "      <td>-0.347270</td>\n",
              "      <td>0.284830</td>\n",
              "      <td>0.075693</td>\n",
              "      <td>-0.062178</td>\n",
              "      <td>-0.389880</td>\n",
              "      <td>0.229020</td>\n",
              "      <td>-0.216170</td>\n",
              "      <td>-0.225620</td>\n",
              "      <td>-0.093918</td>\n",
              "      <td>-0.803750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.680470</td>\n",
              "      <td>-0.039263</td>\n",
              "      <td>0.30186</td>\n",
              "      <td>-0.177920</td>\n",
              "      <td>0.429620</td>\n",
              "      <td>0.032246</td>\n",
              "      <td>-0.41376</td>\n",
              "      <td>0.132280</td>\n",
              "      <td>-0.298470</td>\n",
              "      <td>-0.085253</td>\n",
              "      <td>0.171180</td>\n",
              "      <td>0.224190</td>\n",
              "      <td>-0.10046</td>\n",
              "      <td>-0.436530</td>\n",
              "      <td>0.33418</td>\n",
              "      <td>0.678460</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>-0.344480</td>\n",
              "      <td>-0.42785</td>\n",
              "      <td>-0.432750</td>\n",
              "      <td>0.559630</td>\n",
              "      <td>0.100320</td>\n",
              "      <td>0.186770</td>\n",
              "      <td>-0.268540</td>\n",
              "      <td>0.037334</td>\n",
              "      <td>-2.0932</td>\n",
              "      <td>0.221710</td>\n",
              "      <td>-0.398680</td>\n",
              "      <td>0.209120</td>\n",
              "      <td>-0.557250</td>\n",
              "      <td>3.8826</td>\n",
              "      <td>0.474660</td>\n",
              "      <td>-0.956580</td>\n",
              "      <td>-0.377880</td>\n",
              "      <td>0.208690</td>\n",
              "      <td>-0.327520</td>\n",
              "      <td>0.127510</td>\n",
              "      <td>0.088359</td>\n",
              "      <td>0.163510</td>\n",
              "      <td>-0.216340</td>\n",
              "      <td>-0.094375</td>\n",
              "      <td>0.018324</td>\n",
              "      <td>0.210480</td>\n",
              "      <td>-0.030880</td>\n",
              "      <td>-0.197220</td>\n",
              "      <td>0.082279</td>\n",
              "      <td>-0.094340</td>\n",
              "      <td>-0.073297</td>\n",
              "      <td>-0.064699</td>\n",
              "      <td>-0.260440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>0.268180</td>\n",
              "      <td>0.143460</td>\n",
              "      <td>-0.27877</td>\n",
              "      <td>0.016257</td>\n",
              "      <td>0.113840</td>\n",
              "      <td>0.699230</td>\n",
              "      <td>-0.51332</td>\n",
              "      <td>-0.473680</td>\n",
              "      <td>-0.330750</td>\n",
              "      <td>-0.138340</td>\n",
              "      <td>0.270200</td>\n",
              "      <td>0.309380</td>\n",
              "      <td>-0.45012</td>\n",
              "      <td>-0.412700</td>\n",
              "      <td>-0.09932</td>\n",
              "      <td>0.038085</td>\n",
              "      <td>0.029749</td>\n",
              "      <td>0.100760</td>\n",
              "      <td>-0.25058</td>\n",
              "      <td>-0.518180</td>\n",
              "      <td>0.345580</td>\n",
              "      <td>0.449220</td>\n",
              "      <td>0.487910</td>\n",
              "      <td>-0.080866</td>\n",
              "      <td>-0.101210</td>\n",
              "      <td>-1.3777</td>\n",
              "      <td>-0.108660</td>\n",
              "      <td>-0.232010</td>\n",
              "      <td>0.012839</td>\n",
              "      <td>-0.465080</td>\n",
              "      <td>3.8463</td>\n",
              "      <td>0.313620</td>\n",
              "      <td>0.136430</td>\n",
              "      <td>-0.522440</td>\n",
              "      <td>0.330200</td>\n",
              "      <td>0.337070</td>\n",
              "      <td>-0.356010</td>\n",
              "      <td>0.324310</td>\n",
              "      <td>0.120410</td>\n",
              "      <td>0.351200</td>\n",
              "      <td>-0.069043</td>\n",
              "      <td>0.368850</td>\n",
              "      <td>0.251680</td>\n",
              "      <td>-0.245170</td>\n",
              "      <td>0.253810</td>\n",
              "      <td>0.136700</td>\n",
              "      <td>-0.311780</td>\n",
              "      <td>-0.632100</td>\n",
              "      <td>-0.250280</td>\n",
              "      <td>-0.380970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>0.330420</td>\n",
              "      <td>0.249950</td>\n",
              "      <td>-0.60874</td>\n",
              "      <td>0.109230</td>\n",
              "      <td>0.036372</td>\n",
              "      <td>0.151000</td>\n",
              "      <td>-0.55083</td>\n",
              "      <td>-0.074239</td>\n",
              "      <td>-0.092307</td>\n",
              "      <td>-0.328210</td>\n",
              "      <td>0.095980</td>\n",
              "      <td>-0.822690</td>\n",
              "      <td>-0.36717</td>\n",
              "      <td>-0.670090</td>\n",
              "      <td>0.42909</td>\n",
              "      <td>0.016496</td>\n",
              "      <td>-0.235730</td>\n",
              "      <td>0.128640</td>\n",
              "      <td>-1.09530</td>\n",
              "      <td>0.433340</td>\n",
              "      <td>0.570670</td>\n",
              "      <td>-0.103600</td>\n",
              "      <td>0.204220</td>\n",
              "      <td>0.078308</td>\n",
              "      <td>-0.427950</td>\n",
              "      <td>-1.7984</td>\n",
              "      <td>-0.278650</td>\n",
              "      <td>0.119540</td>\n",
              "      <td>-0.126890</td>\n",
              "      <td>0.031744</td>\n",
              "      <td>3.8631</td>\n",
              "      <td>-0.177860</td>\n",
              "      <td>-0.082434</td>\n",
              "      <td>-0.626980</td>\n",
              "      <td>0.264970</td>\n",
              "      <td>-0.057185</td>\n",
              "      <td>-0.073521</td>\n",
              "      <td>0.461030</td>\n",
              "      <td>0.308620</td>\n",
              "      <td>0.124980</td>\n",
              "      <td>-0.486090</td>\n",
              "      <td>-0.008027</td>\n",
              "      <td>0.031184</td>\n",
              "      <td>-0.365760</td>\n",
              "      <td>-0.426990</td>\n",
              "      <td>0.421640</td>\n",
              "      <td>-0.116660</td>\n",
              "      <td>-0.507030</td>\n",
              "      <td>-0.027273</td>\n",
              "      <td>-0.532850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>0.217050</td>\n",
              "      <td>0.465150</td>\n",
              "      <td>-0.46757</td>\n",
              "      <td>0.100820</td>\n",
              "      <td>1.013500</td>\n",
              "      <td>0.748450</td>\n",
              "      <td>-0.53104</td>\n",
              "      <td>-0.262560</td>\n",
              "      <td>0.168120</td>\n",
              "      <td>0.131820</td>\n",
              "      <td>-0.249090</td>\n",
              "      <td>-0.441850</td>\n",
              "      <td>-0.21739</td>\n",
              "      <td>0.510040</td>\n",
              "      <td>0.13448</td>\n",
              "      <td>-0.431410</td>\n",
              "      <td>-0.031230</td>\n",
              "      <td>0.206740</td>\n",
              "      <td>-0.78138</td>\n",
              "      <td>-0.201480</td>\n",
              "      <td>-0.097401</td>\n",
              "      <td>0.160880</td>\n",
              "      <td>-0.618360</td>\n",
              "      <td>-0.185040</td>\n",
              "      <td>-0.124610</td>\n",
              "      <td>-2.2526</td>\n",
              "      <td>-0.223210</td>\n",
              "      <td>0.504300</td>\n",
              "      <td>0.322570</td>\n",
              "      <td>0.153130</td>\n",
              "      <td>3.9636</td>\n",
              "      <td>-0.713650</td>\n",
              "      <td>-0.670120</td>\n",
              "      <td>0.283880</td>\n",
              "      <td>0.217380</td>\n",
              "      <td>0.144330</td>\n",
              "      <td>0.259260</td>\n",
              "      <td>0.234340</td>\n",
              "      <td>0.427400</td>\n",
              "      <td>-0.444510</td>\n",
              "      <td>0.138130</td>\n",
              "      <td>0.369730</td>\n",
              "      <td>-0.642890</td>\n",
              "      <td>0.024142</td>\n",
              "      <td>-0.039315</td>\n",
              "      <td>-0.260370</td>\n",
              "      <td>0.120170</td>\n",
              "      <td>-0.043782</td>\n",
              "      <td>0.410130</td>\n",
              "      <td>0.179600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"</th>\n",
              "      <td>0.257690</td>\n",
              "      <td>0.456290</td>\n",
              "      <td>-0.76974</td>\n",
              "      <td>-0.376790</td>\n",
              "      <td>0.592720</td>\n",
              "      <td>-0.063527</td>\n",
              "      <td>0.20545</td>\n",
              "      <td>-0.573850</td>\n",
              "      <td>-0.290090</td>\n",
              "      <td>-0.136620</td>\n",
              "      <td>0.327280</td>\n",
              "      <td>1.471900</td>\n",
              "      <td>-0.73681</td>\n",
              "      <td>-0.120360</td>\n",
              "      <td>0.71354</td>\n",
              "      <td>-0.460980</td>\n",
              "      <td>0.652480</td>\n",
              "      <td>0.488870</td>\n",
              "      <td>-0.51558</td>\n",
              "      <td>0.039951</td>\n",
              "      <td>-0.343070</td>\n",
              "      <td>-0.014087</td>\n",
              "      <td>0.864880</td>\n",
              "      <td>0.354600</td>\n",
              "      <td>0.799900</td>\n",
              "      <td>-1.4995</td>\n",
              "      <td>-1.815300</td>\n",
              "      <td>0.411280</td>\n",
              "      <td>0.239210</td>\n",
              "      <td>-0.431390</td>\n",
              "      <td>3.6623</td>\n",
              "      <td>-0.798340</td>\n",
              "      <td>-0.545380</td>\n",
              "      <td>0.169430</td>\n",
              "      <td>-0.820170</td>\n",
              "      <td>-0.346100</td>\n",
              "      <td>0.694950</td>\n",
              "      <td>-1.225600</td>\n",
              "      <td>-0.179920</td>\n",
              "      <td>-0.057474</td>\n",
              "      <td>0.030498</td>\n",
              "      <td>-0.395430</td>\n",
              "      <td>-0.385150</td>\n",
              "      <td>-1.000200</td>\n",
              "      <td>0.087599</td>\n",
              "      <td>-0.310090</td>\n",
              "      <td>-0.346770</td>\n",
              "      <td>-0.314380</td>\n",
              "      <td>0.750040</td>\n",
              "      <td>0.970650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>'s</th>\n",
              "      <td>0.237270</td>\n",
              "      <td>0.404780</td>\n",
              "      <td>-0.20547</td>\n",
              "      <td>0.588050</td>\n",
              "      <td>0.655330</td>\n",
              "      <td>0.328670</td>\n",
              "      <td>-0.81964</td>\n",
              "      <td>-0.232360</td>\n",
              "      <td>0.274280</td>\n",
              "      <td>0.242650</td>\n",
              "      <td>0.054992</td>\n",
              "      <td>0.162960</td>\n",
              "      <td>-1.25550</td>\n",
              "      <td>-0.086437</td>\n",
              "      <td>0.44536</td>\n",
              "      <td>0.096561</td>\n",
              "      <td>-0.165190</td>\n",
              "      <td>0.058378</td>\n",
              "      <td>-0.38598</td>\n",
              "      <td>0.086977</td>\n",
              "      <td>0.003387</td>\n",
              "      <td>0.550950</td>\n",
              "      <td>-0.776970</td>\n",
              "      <td>-0.620960</td>\n",
              "      <td>0.092948</td>\n",
              "      <td>-2.5685</td>\n",
              "      <td>-0.677390</td>\n",
              "      <td>0.101510</td>\n",
              "      <td>-0.486430</td>\n",
              "      <td>-0.057805</td>\n",
              "      <td>3.1859</td>\n",
              "      <td>-0.017554</td>\n",
              "      <td>-0.161380</td>\n",
              "      <td>0.055486</td>\n",
              "      <td>-0.258850</td>\n",
              "      <td>-0.339380</td>\n",
              "      <td>-0.199280</td>\n",
              "      <td>0.260490</td>\n",
              "      <td>0.104780</td>\n",
              "      <td>-0.559340</td>\n",
              "      <td>-0.123420</td>\n",
              "      <td>0.659610</td>\n",
              "      <td>-0.518020</td>\n",
              "      <td>-0.829950</td>\n",
              "      <td>-0.082739</td>\n",
              "      <td>0.281550</td>\n",
              "      <td>-0.423000</td>\n",
              "      <td>-0.273780</td>\n",
              "      <td>-0.007901</td>\n",
              "      <td>-0.030231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>for</th>\n",
              "      <td>0.152720</td>\n",
              "      <td>0.361810</td>\n",
              "      <td>-0.22168</td>\n",
              "      <td>0.066051</td>\n",
              "      <td>0.130290</td>\n",
              "      <td>0.370750</td>\n",
              "      <td>-0.75874</td>\n",
              "      <td>-0.447220</td>\n",
              "      <td>0.225630</td>\n",
              "      <td>0.102080</td>\n",
              "      <td>0.054225</td>\n",
              "      <td>0.134940</td>\n",
              "      <td>-0.43052</td>\n",
              "      <td>-0.213400</td>\n",
              "      <td>0.56139</td>\n",
              "      <td>-0.214450</td>\n",
              "      <td>0.077974</td>\n",
              "      <td>0.101370</td>\n",
              "      <td>-0.51306</td>\n",
              "      <td>-0.402950</td>\n",
              "      <td>0.406390</td>\n",
              "      <td>0.233090</td>\n",
              "      <td>0.206960</td>\n",
              "      <td>-0.126680</td>\n",
              "      <td>-0.506340</td>\n",
              "      <td>-1.7131</td>\n",
              "      <td>0.077183</td>\n",
              "      <td>-0.391380</td>\n",
              "      <td>-0.105940</td>\n",
              "      <td>-0.237430</td>\n",
              "      <td>3.9552</td>\n",
              "      <td>0.665960</td>\n",
              "      <td>-0.618410</td>\n",
              "      <td>-0.326800</td>\n",
              "      <td>0.370210</td>\n",
              "      <td>0.257640</td>\n",
              "      <td>0.389770</td>\n",
              "      <td>0.271210</td>\n",
              "      <td>0.043024</td>\n",
              "      <td>-0.343220</td>\n",
              "      <td>0.020339</td>\n",
              "      <td>0.214200</td>\n",
              "      <td>0.044097</td>\n",
              "      <td>0.140030</td>\n",
              "      <td>-0.200790</td>\n",
              "      <td>0.074794</td>\n",
              "      <td>-0.360760</td>\n",
              "      <td>0.433820</td>\n",
              "      <td>-0.084617</td>\n",
              "      <td>0.121400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f23ee894-8073-4926-a39c-0f9b7d09c1c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f23ee894-8073-4926-a39c-0f9b7d09c1c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f23ee894-8073-4926-a39c-0f9b7d09c1c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           1         2        3   ...        48        49        50\n",
              "0                                 ...                              \n",
              "the  0.418000  0.249680 -0.41242  ... -0.184110 -0.115140 -0.785810\n",
              ",    0.013441  0.236820 -0.16899  ... -0.566570  0.044691  0.303920\n",
              ".    0.151640  0.301770 -0.16763  ... -0.356520  0.016413  0.102160\n",
              "of   0.708530  0.570880 -0.47160  ... -0.225620 -0.093918 -0.803750\n",
              "to   0.680470 -0.039263  0.30186  ... -0.073297 -0.064699 -0.260440\n",
              "and  0.268180  0.143460 -0.27877  ... -0.632100 -0.250280 -0.380970\n",
              "in   0.330420  0.249950 -0.60874  ... -0.507030 -0.027273 -0.532850\n",
              "a    0.217050  0.465150 -0.46757  ... -0.043782  0.410130  0.179600\n",
              "\"    0.257690  0.456290 -0.76974  ... -0.314380  0.750040  0.970650\n",
              "'s   0.237270  0.404780 -0.20547  ... -0.273780 -0.007901 -0.030231\n",
              "for  0.152720  0.361810 -0.22168  ...  0.433820 -0.084617  0.121400\n",
              "\n",
              "[11 rows x 50 columns]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your code HERE - It should execute as expected! \n",
        "# (Search for a pandas functionality that can help you do this!)\n",
        "df.head(11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3jGb0YvSFGJ"
      },
      "source": [
        "## \\# 2. Words Similarity\n",
        "\n",
        "Similar words have similar embeddings (or vector values). We can use cosine similarity i.e. cos(u,v) = u.v/(|u||v|) to measure vector similarity. u.v is dot product of vectors, |u| is L2 norm of u. Remember, we spoke about computing similarity based on cosine-similarity (as another alternative to correlation) in class?\n",
        "\n",
        "1. Normalize matrix df by norm of word vectors. \n",
        "1. Define a function to find words similarity to a given word.\n",
        "1. Use the function defined to find the word in examples that is most similar to \"happy\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDlWoNebSFGK",
        "outputId": "d9016300-c79c-461d-cd49-3814bc0cc379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of vector norm: 400000 , Each Vector will contribute towards an element of norm vector\n",
            "dimension of normalised df:  (400000, 50)\n"
          ]
        }
      ],
      "source": [
        "# Calculate norm of word vectors\n",
        "vector_norm = np.sqrt(np.square(df).sum(axis=1))\n",
        "\n",
        "# What would be the dimension of the vector_norm array?\n",
        "print(\"length of vector norm:\", len(vector_norm), \", Each Vector will contribute towards an element of norm vector\")\n",
        "# Normalize matrix df by norm using .div()\n",
        "dfn = df.div(vector_norm, axis = 0)\n",
        "print(\"dimension of normalised df: \", dfn.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCzDSRfsSFGK",
        "outputId": "0cbb79c0-5683-456a-d178-4ee7e1ada4a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "glad            0.865877\n",
            "hardly          0.816272\n",
            "probably        0.747581\n",
            "bad             0.708395\n",
            "sad             0.689063\n",
            "healthy         0.640579\n",
            "word            0.599150\n",
            "cheerful        0.575719\n",
            "upset           0.566424\n",
            "joyful          0.555032\n",
            "close           0.554029\n",
            "ill             0.522978\n",
            "evil            0.452148\n",
            "disco           0.324669\n",
            "beaming         0.289510\n",
            "cleaning        0.246022\n",
            "distribution    0.160149\n",
            "radiant         0.134971\n",
            "ephemeral       0.132886\n",
            "maths          -0.011991\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Define a function to find words similar to a given word in a normalized dataframe\n",
        "def find_word_similarity(word, examples,dataframe):\n",
        "    \"\"\"\n",
        "    Input: word - one string\n",
        "    examples - List of strings\n",
        "    dataframe - An indexed normalized dataframe\n",
        "    \"\"\"\n",
        "    ## YOUR CODE HERE\n",
        "    dotwithexample = dataframe.loc[examples].dot(dataframe.loc[word])\n",
        "    dotwithexample2 = dotwithexample.sort_values(ascending = False)\n",
        "    # Calculate dot product of each word in examples to the given word, sorted by value high to low\n",
        "    # Once you have the sorted values of dot products (notice because of normalization, the dot product is the cosine similarity!),\n",
        "    # obtain the words corresponding to the sorted values and call it similar_words\n",
        "    similar_words = dotwithexample2\n",
        "\n",
        "    # Return words similar to the given word\n",
        "    return similar_words\n",
        "    \n",
        "examples = [\"sad\", \"bad\", \"evil\", \"healthy\", \"ill\",\n",
        "            \"beaming\", \"cheerful\", \"joyful\", \"radiant\", \"glad\", \"upset\",\n",
        "            \"disco\", \"probably\", \"hardly\", \"ephemeral\", \"close\", \"cleaning\", \n",
        "            \"maths\", \"word\", \"distribution\"]\n",
        "\n",
        "# Use above function to calculate examples' similarity to happy (both \"happy\" and words in examples are in dfn)\n",
        "print ( find_word_similarity(\"happy\", examples,dfn) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u5QTMJISFGK",
        "outputId": "b8ee60ab-7e7a-4bff-c911-1e0661bb6506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     0\n",
            "glad          0.865877\n",
            "hardly        0.816272\n",
            "probably      0.747581\n",
            "bad           0.708395\n",
            "sad           0.689063\n",
            "healthy       0.640579\n",
            "word          0.599150\n",
            "cheerful      0.575719\n",
            "upset         0.566424\n",
            "joyful        0.555032\n",
            "close         0.554029\n",
            "ill           0.522978\n",
            "evil          0.452148\n",
            "disco         0.324669\n",
            "beaming       0.289510\n",
            "cleaning      0.246022\n",
            "distribution  0.160149\n",
            "radiant       0.134971\n",
            "ephemeral     0.132886\n",
            "maths        -0.011991\n"
          ]
        }
      ],
      "source": [
        "examples = [\"sad\", \"bad\", \"evil\", \"healthy\", \"ill\",\n",
        "            \"beaming\", \"cheerful\", \"joyful\", \"radiant\", \"glad\", \"upset\",\n",
        "            \"disco\", \"probably\", \"hardly\", \"ephemeral\", \"close\", \"cleaning\", \n",
        "            \"maths\", \"word\", \"distribution\"]\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity2 = pd.DataFrame(cosine_similarity(np.array(df.loc['happy']).reshape(1,50),np.array(df.loc[examples])),columns = examples)\n",
        "similarity2 = similarity2.T.sort_values(by=[0], ascending=False)\n",
        "print(similarity2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK8XKjczSFGK"
      },
      "source": [
        "## \\# 3. Goodness of similarity\n",
        "Comment on the how good the glove embeddings are on finding similar words to a given word using cosine similarity? Glove and word2vec embeddings are based on co-occurence of words in senetences across hundreds of thousands of documents on the web. Would this help explain your observations on word similarity?\n",
        "What if you replace happy with sad, how do the results change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttC-HD2uLwLi"
      },
      "source": [
        "**Interpretation:**\n",
        "Maths got negative similarity with happy. In general, it is often seen that most of the people don't like math or have troubles with math. This similarity seems appropriate. similarly, glad got maximum similarity with happy which also makes sense. The similarity between happy - (hardly and sad) establishes the fact the Glove considers co-occurences in global and well as local corpus of words. sentiments are not factored in this approach of similarity. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiFGS83Mo1fG",
        "outputId": "b4e342cb-cd75-4f32-c753-d2aadf386cb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "hardly          0.707888\n",
            "happy           0.689063\n",
            "bad             0.664580\n",
            "glad            0.644321\n",
            "joyful          0.587249\n",
            "word            0.559652\n",
            "probably        0.535477\n",
            "cheerful        0.531766\n",
            "upset           0.501137\n",
            "ill             0.487844\n",
            "evil            0.401493\n",
            "healthy         0.372976\n",
            "disco           0.362746\n",
            "close           0.311398\n",
            "radiant         0.244188\n",
            "ephemeral       0.234799\n",
            "beaming         0.194717\n",
            "cleaning        0.135319\n",
            "maths           0.030129\n",
            "distribution    0.008951\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "examples1 = [\"happy\", \"bad\", \"evil\", \"healthy\", \"ill\",\n",
        "            \"beaming\", \"cheerful\", \"joyful\", \"radiant\", \"glad\", \"upset\",\n",
        "            \"disco\", \"probably\", \"hardly\", \"ephemeral\", \"close\", \"cleaning\", \n",
        "            \"maths\", \"word\", \"distribution\"]\n",
        "print ( find_word_similarity(\"sad\", examples1,dfn) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xkNfwBbT8bb"
      },
      "source": [
        "similarity between math and sad increased which again justify the co-occurence situation. similarity of disco with sad is greater than with happy, this might be because people tend to go to a disco when they are sad and hence, these words are used more frequently. Cleaning has higher similarity to Happy. \n",
        "\n",
        "**Takeaway:** Similarity using GloVe embedding find the words which have similar connotation but doesn't captures the sentiment invloved with the words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtPrwU9ISFGL"
      },
      "source": [
        "## \\# 4. Correlations\n",
        "(This question is more of a reflection and building your intuition on how correlations we spoke in class connects to a real-world data set -  Open ended!)\n",
        "What are some of the most correlated words from the similarity search you did earlier to the word \"happy\" and \"sad\". Likewise, what are some of the most uncorrelated words to \"happy\" and \"sad\". Does this make sense? How would you improve the results ? If \"happy\" were a random variable and \"sad\" was a random variable - What factors make the correlation between \"happy\" and \"sad\" (as you computed above) high?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KKAUSayXBGT"
      },
      "source": [
        "#### Most similar words to \"happy\" are(>70): \n",
        "1. glad\n",
        "2. hardly \n",
        "3. probably \n",
        "4. bad\n",
        "\n",
        "#### Most uncorrelated words to \"happy\" are(<0.15): \n",
        "1. maths\n",
        "2. ephemeral  \n",
        "3. radiant  \n",
        "4. distribution\n",
        "\n",
        "#### Most similar words to \"sad\" are(>60): \n",
        "1. hardly\n",
        "2. happy \n",
        "3. bad\n",
        "4. glad\n",
        "\n",
        "#### Most uncorrelated words to \"sad\" are(<0.15): \n",
        "1. distribution \n",
        "2. maths\n",
        "3. cleaning \n",
        "\n",
        "some of these are doesn't make sense as they represents similar category of feeling but they don't necessarily represents same feeling. But vectors in GloVe embedding can be great starting point. \n",
        "\n",
        "1. It can also be improved by increasing the dimensinality of the each vector. \n",
        "2. Results of glove embedding can be improved by factoring sentiments in the similarity calculation. \n",
        "\n",
        "If \"Happy\" and \"sad\" where random variables, 2 factors will result in high correaltion: \n",
        "1. Probability of co-occurence. They belong to same category \"emotions\". so, there is high chance of using these words in a particular context. \n",
        "2. It is also imperative that, given a sentence happy and sad can be used interchangably to represent two different emotions. example, student was happy with the output of his code. student was sad with the output of his code. all the other words remain same around happy and sad. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlrOcmpKSFGL"
      },
      "source": [
        "## \\# 5. Find nearest neighbourhood\n",
        "\n",
        "It is helpful to compute the nearest neighbors to a word based on the cosine similarity that we defined earlier, so that given a word we can compute which are the other words which are most similar to it. Sometimes, the nearest neighbors according to this metric reveal overlap of concepts or topics that a word shares. E.g. government might be related to the word politics because they both share topics related to public policy, politicians, parties, elections, etc. The idea is whatever embeddings we are using - word2vec or glove is \"hopefully\" able to capture these correlations right!\n",
        "\n",
        "1. Define a function to find the top n similar words to a given word. You can use either dot product of vectors or cosine_simialrity function. Note the search space for words is coming from your pandas data frame (so unlike the similarity problem we worked on earlier, we are not restricted to only a few words to search from - the search space here is the entire vocab captured in your data frame).\n",
        "1. Find 20 nearest neighborhood for words 'duck' and 'animal'.\n",
        "1. Find neighborhood intersection of 'duck' and 'animal', to find which words are similar to both 'duck' and 'animal'. This is also related to a similarity measure called \"Jaccard Similarity\" - Read up on this here: https://en.wikipedia.org/wiki/Jaccard_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p-UX9pxSFGL",
        "outputId": "20adc635-466f-4520-e569-c96c8d86d774"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          duck    animal\n",
            "pig   0.739596  0.735997\n",
            "cat   0.703251  0.684206\n",
            "dog   0.693291  0.725226\n",
            "fish  0.676019  0.728633\n",
            "meat  0.655953  0.675570\n",
            "bird  0.643547  0.800324\n",
            "\n",
            "word labels are:\n",
            " ['duck', 'animal', 'pig', 'cat', 'dog', 'fish', 'meat', 'bird']\n"
          ]
        }
      ],
      "source": [
        "# define a function to find the top n similar words to a given word in the 'df'\n",
        "\n",
        "# PART 1\n",
        "def find_most_similar(df, word, n):\n",
        "    \"\"\"\n",
        "    INPUT: \n",
        "    df: Given Data frame\n",
        "    word: String\n",
        "    n: Number of similar words to return\n",
        "    OUTPUT:\n",
        "    the list of similar words to return\n",
        "    \"\"\"\n",
        "    ## YOUR CODE HERE\n",
        "    # define and compute the most similar words\n",
        "    # Use a similarity measure like cosine similarity (like earlier) to do so\n",
        "    df = df.loc[~df.index.duplicated()]\n",
        "    words = np.array(df.index)\n",
        "    similarity = pd.DataFrame(cosine_similarity(np.array(df.loc[word]).reshape(1,50),np.array(df.loc[words])),columns = words)\n",
        "    similarity = similarity.T.sort_values(by=[0], ascending=False)\n",
        "    similarity_sorted = similarity.head(n)\n",
        "    \n",
        "    return similarity_sorted\n",
        "\n",
        "\n",
        "# PART 2\n",
        "# find top 40 similar words to duck\n",
        "simil1 = find_most_similar(df, \"duck\", 40)\n",
        "# find top 40 similar words to animal\n",
        "simil2 = find_most_similar(df, \"animal\", 40)\n",
        "\n",
        "# PART 3\n",
        "# find the intersection of simil1 and simil2\n",
        "#intersection =  (concat function of pandas is helpful here)\n",
        "combined = pd.concat([simil1, simil2], axis=1)\n",
        "combined.columns = ['duck', 'animal']\n",
        "combined_final = combined.dropna(how='any')\n",
        "print (combined_final)\n",
        "\n",
        "word_labels = ['duck', 'animal'] + list(combined_final.index)\n",
        "print(\"\\nword labels are:\\n\", word_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNOY8vrPSFGL"
      },
      "source": [
        "## \\# 6 Word analogies\n",
        "\n",
        "Suppose you know the word vectors for King, Man and Woman. What is your intuitive answer for the 'riddle', King - Man + Woman = ? \n",
        "Let's go through below steps to derive the answer for this 'riddle' using the word embeddings.\n",
        "\n",
        "1. Use vector arithmetic to define a new vector which equals to k - m + w (e.g. king, man and woman combination).\n",
        "2. Calculate similarity of all the words in the corpus to the new vector and sort them by their similarity high to low. \n",
        "3. Return the top n vectors which have the highest similarity to the new vector.\n",
        "1. Find the answers for the riddles, \n",
        "    1. good:bad::up:?\n",
        "    1. germany:merkel::america:?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No_mssuySFGL",
        "outputId": "10929c6e-7c68-4b1f-9c08-4c6284f8e1a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "King - Man + Woman =\n",
            "                  0\n",
            "queen     0.860958\n",
            "daughter  0.768451\n",
            "prince    0.764070\n",
            "throne    0.763497\n",
            "princess  0.751273\n",
            "\n",
            "good:bad::up:?\n",
            "                  0\n",
            "down      0.850167\n",
            "falling   0.813844\n",
            "out       0.792837\n",
            "dropping  0.782064\n",
            "off       0.778428\n",
            "\n",
            "germany:merkel::america:?\n",
            "                 0\n",
            "obama    0.694289\n",
            "barack   0.682594\n",
            "hillary  0.660910\n",
            "bush     0.657911\n",
            "clinton  0.655765\n"
          ]
        }
      ],
      "source": [
        "def solve_riddle(x, y, a, n, dataframe):\n",
        "    ## YOUR CODE HERE\n",
        "    # calculate the vector of a + y - x, where a, x, y are in dataframe\n",
        "    vec_a = np.array(dataframe.loc[a])\n",
        "    vec_x = np.array(dataframe.loc[x])\n",
        "    vec_y = np.array(dataframe.loc[y])\n",
        "\n",
        "    cal_vec = vec_a + vec_y - vec_x\n",
        "    # calculate distance of words in dataframe to cal_vec, sorted by similarity high to low\n",
        "    df = dataframe.loc[~dataframe.index.duplicated()]\n",
        "    df = df.drop([a, x, y])\n",
        "    words = np.array(df.index)\n",
        "    similarity = pd.DataFrame(cosine_similarity(np.array(cal_vec).reshape(1,50),\n",
        "                                                np.array(df.loc[words])),columns = words)\n",
        "    similarity = similarity.T.sort_values(by=[0], ascending=False)\n",
        "    similarity_sorted = similarity.head(n)\n",
        "\n",
        "    # return top n words and distance that closest to cal_vec\n",
        "    return similarity_sorted \n",
        "\n",
        "# Call the solve_riddle function to compute the top answers\n",
        "print (\"King - Man + Woman =\\n\", solve_riddle(\"man\", \"woman\", \"king\", 5,df) )\n",
        "\n",
        "## YOUR CODE HERE\n",
        "# Solve the other two riddles\n",
        "# good:bad::up:?\n",
        "# # PART 4\n",
        "print (\"\\ngood:bad::up:?\\n\", solve_riddle(\"good\", \"bad\", \"up\", 5,df) )\n",
        "\n",
        "# # germany:merkel::america:?\n",
        "print (\"\\ngermany:merkel::america:?\\n\",solve_riddle(\"germany\", \"merkel\", \"america\", 5,df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWfjxJAXoJ9q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EEP596 Prog4_MayankSubmission.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0806d8f4898b409bb81534cc6b50053c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa6e5fb15e4343458fcd67c84bfd533f",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e762cd39392f431db0aef73a9c503dc5",
            "value": 1000
          }
        },
        "0d21c53ae0a44311a7930c5b37aff309": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "441d9c2189ff42caaecc091a99791f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "669fe5d1149d4df88b2f1f2090f13742": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72cbcc25d85245abbef644e9b35639ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a920fb99486b4ac1adf95f9bb629fce1",
            "placeholder": "​",
            "style": "IPY_MODEL_669fe5d1149d4df88b2f1f2090f13742",
            "value": " 1000/1000 [05:58&lt;00:00,  2.92it/s]"
          }
        },
        "a920fb99486b4ac1adf95f9bb629fce1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa6e5fb15e4343458fcd67c84bfd533f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b21967a5652a4ad48c17f0e7676c1646": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d21c53ae0a44311a7930c5b37aff309",
            "placeholder": "​",
            "style": "IPY_MODEL_441d9c2189ff42caaecc091a99791f39",
            "value": "100%"
          }
        },
        "ce92f51e95d34eddb0db72f520d3a72e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e762cd39392f431db0aef73a9c503dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f28d1247f0ce4c16a494a95d48f54491": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b21967a5652a4ad48c17f0e7676c1646",
              "IPY_MODEL_0806d8f4898b409bb81534cc6b50053c",
              "IPY_MODEL_72cbcc25d85245abbef644e9b35639ed"
            ],
            "layout": "IPY_MODEL_ce92f51e95d34eddb0db72f520d3a72e"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
